{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d5d4d1-b0de-44b0-9403-7d6f1afe2ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/visnja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/visnja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud as cloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as stopwords\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_lg\n",
    "nlp =  en_core_web_lg.load()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['reuters','cnbc','year','last','banks','first','one','two','billion','due','rose','next','global','end','investors','lower','risk','back', 'week','however','policy', 'editing', 'figures','currencies','coronavirus','analysts','interest',\n",
    "                   'level','currency','economy','long','term','likely','reporting','resistance','central','bank', 'tuesday', 'friday', 'march','april','june','july','august','september','october','november','december','january','february','barely','enough',\n",
    "                   'expected','ahead','data','writing','around','today','index','reported','price','prices','inflation', 'market','markets', 'month', 'could','rate','rates','time',\n",
    "                   'info','said','would','may','since','also','support','new','higher','day','high','low','trade','trading','wednesday','thursday','monday','economic','calendar'])\n",
    "\n",
    "stop_words.extend(['january','february','march','april','may','june','july','august','september','october','november','december','monday','tuesday',\n",
    "                  'wednesday','thursday','friday','saturday','sunday','will','day','today','week','weeks','yesterday','tomorrow'])\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import pyLDAvis as pyldavis\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "connection=MongoClient(\"mongodb://localhost:27017/crawler.contents\")\n",
    "\n",
    "db=connection.get_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910aca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d516074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "articles = pd.DataFrame(list(db.contents.find()))\n",
    "\n",
    "\n",
    "articles = articles.drop(columns=['visited','created_at','contentType','date','icon','_id'])\n",
    "articles.head()\n",
    "articles = articles.dropna(how='any',axis=0)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean(txt):\n",
    "    excluded_tags = {\"VERB\", \"ADV\",\"ADJ\",\"NUM\",\"PRON\",\"ADP\",\"AUX\",\"CCONJ\",\"DET\",\"INTJ\",\"PART\"}\n",
    "    new_sentence = []\n",
    "    for token in nlp(txt):\n",
    "        if token.pos_ not in excluded_tags:\n",
    "            new_sentence.append(token.text)\n",
    "    res = \" \".join(new_sentence)\n",
    "    res = re.sub(r'[,\\.!?]','',txt)\n",
    "    res = str(res.lower())\n",
    "    res = re.sub(r'\\d* min read', '', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'by reuters staff', '', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'australian dollar', 'aud', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'us dollar', 'usd', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'british pound', 'gbp', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'canadian dollar', 'cad', str(res), flags=re.IGNORECASE)\n",
    "    res = re.sub(r'euro ', 'eur ', str(res), flags=re.IGNORECASE)\n",
    "#     new_sentence = []\n",
    "#     for token in nlp(res):\n",
    "#         if token.pos_ not in excluded_tags:\n",
    "#             new_sentence.append(token.text)\n",
    "    res = re.sub(r'[^\\w\\s]', '', res)\n",
    "    res = re.sub(r'\\d*', '', res)\n",
    "    \n",
    "#     res = re.sub(r'[^\\d]', '', res)\n",
    "#     lst_txt = res.split()\n",
    "#     ps = nltk.stem.porter.PorterStemmer()\n",
    "#     lst_txt = [ps.stem(word) for word in lst_txt]\n",
    "#     lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#     lst_txt = [lem.lemmatize(word) for word in lst_txt]\n",
    "#     lst_txt = remove_stopwords(lst_txt)\n",
    "#     txt = \" \".join(txt)\n",
    "    return res\n",
    "\n",
    "articles['text_processed'] = articles['body'].map(lambda x: [clean(y)  if isinstance(x,list) else [] for y in list(x)])\n",
    "\n",
    "articles['text_processed_join'] = articles['text_processed'].map(lambda x:  \" \".join(x))\n",
    "print(articles['text_processed_join'][0])\n",
    "\n",
    "long_string = ','.join(list(articles['text_processed_join'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = cloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = articles['text_processed_join'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/01ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pyLDAvis.save_html(LDAvis_prepared, './results/01ldavis_prepared_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
