{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d5d4d1-b0de-44b0-9403-7d6f1afe2ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/visnja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/visnja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/visnja/anaconda3/envs/python3.7-env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud as cloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as stopwords\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_lg\n",
    "nlp =  en_core_web_lg.load()\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import pyLDAvis as pyldavis\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "connection=MongoClient(\"mongodb://localhost:27017/crawler.contents\")\n",
    "\n",
    "db=connection.get_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d516074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "articles = pd.DataFrame(list(db.contents.find()))\n",
    "articles = articles.drop(columns=['visited','alternateImageUrl','created_at','contentType','date','icon','_id','publishedAt','source','url'])\n",
    "articles = articles.dropna(how='any',axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.lang.en import English\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17e2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('AUD', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]aud'}}],\n",
    "    [{'LOWER':{'REGEX':'aud[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'aud'}],\n",
    "    [{'LOWER':'aussie'}],\n",
    "    [{\"LOWER\":'the','OP':'?'},{\"LOWER\":'australian'},{'LOWER':{'REGEX':'dollar[s]?'}}]\n",
    "])\n",
    "matcher.add('USD', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]usd'}}],\n",
    "    [{'LOWER':{'REGEX':'usd[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':{'REGEX':'[-]?the'}},{\"LOWER\":'dollar'}],\n",
    "    [{\"LOWER\":'us'},{'LOWER':{'REGEX':'dollar[s]?'}}],\n",
    "    [{\"LOWER\":'u.s.'},{'LOWER':{'REGEX':'dollar[s]?'}}],\n",
    "    [{\"POS\":'ADJ'},{'LOWER':{'REGEX':'dollar[s]?'}}],\n",
    "    [{\"DEP\": \"compound\",\"OP\": \"?\"},{'LOWER':{'REGEX':'dollar[s]?'}}],\n",
    "    [{\"LOWER\":\"usd\"}]\n",
    "])\n",
    "matcher.add('EUR', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]eur'}}],\n",
    "    [{'LOWER':{'REGEX':'eur[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'eur'}],\n",
    "    [{'LOWER':'the','OP':'?'},{'LOWER':{'REGEX':'euro[s]?[.]?'}}]\n",
    "])\n",
    "matcher.add('GBP', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]gbp'}}],\n",
    "    [{'LOWER':{'REGEX':'gbp[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'gbp'}],\n",
    "    [{'LOWER':'the','OP':'?'},{'LOWER':'british','OP':'?'},{'LOWER':{'REGEX':'pound[s]?'}}],\n",
    "    [{'LOWER':{'REGEX':'[-]?sterling[s]?'}}]\n",
    "])\n",
    "matcher.add('CHF', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]chf'}}],\n",
    "    [{'LOWER':{'REGEX':'chf[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'chf'}],\n",
    "    [{'LOWER':'swissie'}],\n",
    "    [{'LOWER':'swiss'},{'LOWER':{'REGEX':'franc[s]?'}}]])\n",
    "matcher.add('CAD', [\n",
    "    [{'LOWER':'cad'}],\n",
    "    [{'LOWER':'cdn'}],\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]cad'}}],\n",
    "    [{'LOWER':{'REGEX':'cad[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'canadian'},{'LOWER':{'REGEX':'dollar[s]?'}}]])\n",
    "matcher.add('CNY', [\n",
    "    [{'LOWER':'cny'}],\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]cny'}}],\n",
    "    [{'LOWER':{'REGEX':'cny[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'the','OP':'?'},{'LOWER':'chinese'},{'LOWER':'yuan'},{\"IS_PUNCT\": True,'OP':'?'}],\n",
    "    [{'LOWER':{'REGEX':'yuan[s]?'}}]\n",
    "                   ])\n",
    "matcher.add('TWD', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]twd'}}],\n",
    "    [{'LOWER':{'REGEX':'twd[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'twd'}],\n",
    "    [{'LOWER':'taiwan'},{'LOWER':'new','OP':'?'},{'LOWER':{'REGEX':'dollar[s]?'}}]\n",
    "    ])\n",
    "matcher.add('NZD', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]nzd'}}],\n",
    "    [{'LOWER':{'REGEX':'nzd[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'nzd'}],\n",
    "    [{'LOWER':'kiwi'}],\n",
    "    [{'LOWER':'new'},{'LOWER':'zealand'},{'LOWER':{'REGEX':'dollar[s]?'}}]\n",
    "])\n",
    "matcher.add('BTC', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]btc'}}],\n",
    "    [{'LOWER':{'REGEX':'btc[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'btc'}],\n",
    "    [{'LOWER':{'REGEX':'bitcoin[s]?'}}]\n",
    "])\n",
    "matcher.add('JPY', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]jpy'}}],\n",
    "    [{'LOWER':{'REGEX':'jpy[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'jpy'}],\n",
    "    [{'LOWER':'japanese'},{'LOWER':{'REGEX':'yen[s]?'}}],\n",
    "    [{'LOWER':'the','OP':'?'},{'LOWER':{'REGEX':'yen[s]?'}}]\n",
    "])\n",
    "matcher.add('TRY', [\n",
    "    [{'LOWER':'try'}],\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]try'}}],\n",
    "    [{'LOWER':{'REGEX':'try[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'turkish'},{'LOWER':{'REGEX':'lira[s]?'}}],\n",
    "    [{'LOWER':{'REGEX':\"turkey's?\"}},{'LOWER':{'REGEX':'lira[s]?'}}],\n",
    "    \n",
    "])\n",
    "matcher.add('ARS', [\n",
    "    [{'LOWER':'ars'}],\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]ars'}}],\n",
    "    [{'LOWER':{'REGEX':'ars[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'argentine'},{'LOWER':{'REGEX':'peso[s]?'}}],\n",
    "    \n",
    "])\n",
    "matcher.add('MXN', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]mxn'}}],\n",
    "    [{'LOWER':{'REGEX':'mxn[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'mxn'}],\n",
    "    [{'LOWER':'mexican'},{'LOWER':{'REGEX':'peso[s]?'}}],\n",
    "    [{'LOWER':'mexican'},{'LOWER':{'REGEX':'currenc[y|ies]?'}}]\n",
    "    \n",
    "])\n",
    "\n",
    "matcher.add('RUB', [\n",
    "    [{'LOWER':'rub'}],\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]rub'}}],\n",
    "    [{'LOWER':{'REGEX':'rub[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'russian'},{'LOWER':{'REGEX':'ruble[s]?'}}],\n",
    "    [{'LOWER':{'REGEX':'ruble[s]?'}}]\n",
    "    \n",
    "])\n",
    "matcher.add('OIL', [\n",
    "    [{'LOWER':'crude'},{'LOWER':{'REGEX':'oil[s]?'}}],\n",
    "])\n",
    "matcher.add('XAU', [\n",
    "    [{'LOWER':{'REGEX':'[a-z][a-z][a-z]xau'}}],\n",
    "    [{'LOWER':{'REGEX':'xau[a-z][a-z][a-z]'}}],\n",
    "    [{'LOWER':'xau'}],\n",
    "    [{'LOWER':'gold'},{'LOWER':{'REGEX':'price[s]?'}}],\n",
    "])\n",
    "matcher.add('INDEX',[\n",
    "    [{'LOWER':'nasdaq100'}],\n",
    "    [{'LOWER':'nasdaq'},{'ORTH':\"100\"}],\n",
    "    [{'LOWER':'s&p500'}],\n",
    "    [{'LOWER':'sp500'}]\n",
    "    [{\"LOWER\": \"sp\"}, {\"ORTH\": \"500\"}],\n",
    "    [{\"LOWER\": \"s&p\"}, {\"ORTH\": \"500\"}],\n",
    "    [{'LOWER':'dow'},{'LOWER':'jones'}],\n",
    "    [{'LOWER':'djia'}],\n",
    "    [{'LOWER':'dow'},{'LOWER':'jones'},{'LOWER':'industrial'},{'LOWER':'average'}],\n",
    "    [{'LOWER':'ftse'},{\"ORTH\": '100','OP':'?'}],\n",
    "    [{'LOWER':'financial'},{'LOWER':'times'},{'LOWER':'stock'},{'LOWER':'exchange'}],\n",
    "    [{'LOWER':'footsie'}],\n",
    "    [{'LOWER':'dax'}],\n",
    "    [{'LOWER':'dax'},{\"ORTH\": '40','OP':'?'}],\n",
    "    [{'LOWER':'german'},{'LOWER':'stock'},{'LOWER':'index'}],\n",
    "    [{'LOWER':'nikkei'},{\"ORTH\": '225','OP':'?'},{'LOWER':'index','OP':'?'}],\n",
    "    [{'LOWER':'nikkei'},{'LOWER':'stock'},{'LOWER':'average'}],\n",
    "    [{'LOWER':'hsi'},],\n",
    "    [{'LOWER':'hang'},{'LOWER':'seng'},{'LOWER':'index'}]\n",
    "])\n",
    "matcher_2 = Matcher(nlp.vocab)\n",
    "matcher_2.add('NASDAQ100',[\n",
    "    [{'LOWER':'nasdaq100'}],\n",
    "    [{'LOWER':'nasdaq'},{'ORTH':\"100\"}]\n",
    "]) \n",
    "matcher_2.add('S&P500',[\n",
    "    [{'LOWER':'s&p500'}],\n",
    "    [{'LOWER':'sp500'}]\n",
    "    [{\"LOWER\": \"sp\"}, {\"ORTH\": \"500\"}],\n",
    "    [{\"LOWER\": \"s&p\"}, {\"ORTH\": \"500\"}]\n",
    "])\n",
    "matcher_2.add('DJIA',[\n",
    "    [{'LOWER':'dow'},{'LOWER':'jones'}],\n",
    "    [{'LOWER':'djia'}],\n",
    "    [{'LOWER':'dow'},{'LOWER':'jones'},{'LOWER':'industrial'},{'LOWER':'average'}]\n",
    "    \n",
    "])\n",
    "matcher_2.add('FTSE',[\n",
    "    [{'LOWER':'ftse'},{\"ORTH\": '100','OP':'?'}],\n",
    "    [{'LOWER':'financial'},{'LOWER':'times'},{'LOWER':'stock'},{'LOWER':'exchange'}],\n",
    "    [{'LOWER':'footsie'}]\n",
    "    \n",
    "])\n",
    "matcher_2.add('DAX',[\n",
    "    [{'LOWER':'dax'}],\n",
    "    [{'LOWER':'dax'},{\"ORTH\": '40','OP':'?'}],\n",
    "    [{'LOWER':'german'},{'LOWER':'stock'},{'LOWER':'index'}]\n",
    "    \n",
    "])\n",
    "matcher_2.add('N225',[\n",
    "    [{'LOWER':'nikkei'},{\"ORTH\": '225','OP':'?'},{'LOWER':'index','OP':'?'}],\n",
    "    [{'LOWER':'nikkei'},{'LOWER':'stock'},{'LOWER':'average'}]\n",
    "    \n",
    "])\n",
    "matcher_2.add('HSI',[\n",
    "    [{'LOWER':'hsi'},],\n",
    "    [{'LOWER':'hang'},{'LOWER':'seng'},{'LOWER':'index'}]\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cases = {}\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases)\n",
    "\n",
    "\n",
    "tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokens(text):\n",
    "    classes = []\n",
    "    tokens = nlp(text)\n",
    "    matches = matcher(tokens)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        classes.append(string_id)\n",
    "        span = tokens[start:end]  # The matched span\n",
    "    return set(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 183\n",
    "# print(articles['summary'][index],match_tokens(articles['summary'][index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['classes']= articles['summary'].map(lambda x: list(match_tokens(x)))\n",
    "\n",
    "articles['classes_body']= articles['body'].map(lambda x: list(match_tokens(\" \".join(x))))\n",
    "\n",
    "\n",
    "\n",
    "articles['classes_title']= articles['title'].map(lambda x: list(match_tokens(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30086/1699386791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmismatch_sum_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmismatch_sum_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "no_summary = []\n",
    "mismatch_sum_title = []\n",
    "mismatch_sum_body = []\n",
    "for idx,article in articles.iterrows():\n",
    "    article['target'] = list(set(article['classes_title'] + article['classes_body']+article['classes']))\n",
    "    if len(article['classes_title'])== 0:\n",
    "        print(article['title'])\n",
    "    if len(article['classes'])== 0:\n",
    "        no_summary.append(idx)\n",
    "    if len(article['classes']) != len(article['classes_title']):\n",
    "        mismatch_sum_title.append(idx)\n",
    "    if len(article['classes']) != len(article['classes_body']):\n",
    "        mismatch_sum_body.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3651\n",
      "11258\n",
      "7890\n",
      "4263\n"
     ]
    }
   ],
   "source": [
    "print(len(no_summary))\n",
    "\n",
    "print(len(articles))\n",
    "\n",
    "print(len(mismatch_sum_body))\n",
    "\n",
    "print(len(mismatch_sum_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2444595808.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_15025/2444595808.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    articles.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
