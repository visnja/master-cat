{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1331230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\igo\\miniconda3\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\igo\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\igo\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igo\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pymongo import MongoClient\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b9c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "#     MODEL = 'bert-base-uncased'\n",
    "#     MODEL = 'ProsusAI/finbert'\n",
    "    MODEL = 'roberta-base'\n",
    "\n",
    "    HIDDEN = 768\n",
    "\n",
    "    MAX_LENGTH = 64\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VALID_BATCH_SIZE = 32\n",
    "\n",
    "    EPOCHS = 10\n",
    "\n",
    "    LR = (2e-5, 3e-5, 5e-5)\n",
    "    EPS = 1e-8\n",
    "\n",
    "    SEED = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection=MongoClient(\"mongodb://192.168.0.18:27017/crawler.contents\")\n",
    "db=connection.get_database()\n",
    "data = pd.DataFrame(list(db.contents.find()))\n",
    "data = data.drop(columns=['visited','created_at','contentType','date','icon'])\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data[(data['classes_summary'].str.len() > 0)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfd7f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _id  \\\n",
      "0     62444779de4f8e740ef17d01   \n",
      "1     62444788de4f8e740ef17d37   \n",
      "2     623d642a6e887c1f5ed1a680   \n",
      "3     62495f2ac461dc595d8961d2   \n",
      "4     623d631a6e887c1f5ed1a4e6   \n",
      "...                        ...   \n",
      "7797  62854d577257d3c8eabe3cfb   \n",
      "7798  624433da0524426478c687a4   \n",
      "7799  626689b139c405c10597fd2b   \n",
      "7800  6249626ba23aaff290bfc687   \n",
      "7801  6244407d4bbbb4df3b921617   \n",
      "\n",
      "                                                  title  \\\n",
      "0                        Euro pauses after recent gains   \n",
      "1        Investors eye NZ inflation, manufacturing data   \n",
      "2     Dollar slides vs yen, euro as U.S. stocks, Tre...   \n",
      "3     Euro Outlook Latest: EUR/USD Slides Ahead of M...   \n",
      "4     Dollar gains vs yen, others after U.S. jobs re...   \n",
      "...                                                 ...   \n",
      "7797  Japanese Yen Technical Analysis: USD/JPY, EUR/...   \n",
      "7798  European Stocks Drop as Yields Tick Higher, EU...   \n",
      "7799  Japan’s finance ministry official denies repor...   \n",
      "7800  Western sanctions on Russia likely to increase...   \n",
      "7801               GBP Extends Gains, Pushes Above 1.35   \n",
      "\n",
      "                                                summary  \\\n",
      "0     The euro has posted small gains in the Wednesd...   \n",
      "1     The New Zealand dollar continues to head highe...   \n",
      "2     The dollar on Wednesday dropped to a one-week ...   \n",
      "3     The US dollar is reclaiming some of its recent...   \n",
      "4     The U.S. dollar rose against some currencies o...   \n",
      "...                                                 ...   \n",
      "7797  The Yen meltdown has continued and this mornin...   \n",
      "7798  European stocks started to sell off amid highe...   \n",
      "7799  TOKYO (Reuters) – A senior Japanese finance mi...   \n",
      "7800  (Reuters) – Western nations will try to find n...   \n",
      "7801  The British pound is up for a third straight d...   \n",
      "\n",
      "                                                    url           source  \\\n",
      "0     https://www.marketpulse.com/20210210/euro-paus...  marketpulse.com   \n",
      "1     https://www.marketpulse.com/20210121/investors...  marketpulse.com   \n",
      "2     https://www.reuters.com/article/uk-global-fore...      reuters.com   \n",
      "3     https://www.dailyfx.com/forex/market_alert/202...      dailyfx.com   \n",
      "4     https://www.reuters.com/article/uk-global-fore...      reuters.com   \n",
      "...                                                 ...              ...   \n",
      "7797  https://www.dailyfx.com/forex/fundamental/dail...      dailyfx.com   \n",
      "7798  https://www.dailyfx.com/forex/market_alert/202...      dailyfx.com   \n",
      "7799  https://www.fxempire.com/news/article/japans-f...     fxempire.com   \n",
      "7800  https://www.fxempire.com/news/article/western-...     fxempire.com   \n",
      "7801  https://www.investing.com/analysis/gbp-extends...    investing.com   \n",
      "\n",
      "                                      alternateImageUrl  \\\n",
      "0                                                   NaN   \n",
      "1                                                   NaN   \n",
      "2     https://static.reuters.com/resources/r/?m=02&d...   \n",
      "3     https://a.c-dn.net/b/0Pygtu/headline_21246581_...   \n",
      "4     https://static.reuters.com/resources/r/?m=02&d...   \n",
      "...                                                 ...   \n",
      "7797    https://a.c-dn.net/b/2Dkl8o/headline_YEN_07.JPG   \n",
      "7798  https://a.c-dn.net/b/3OR9io/headline_stock-exc...   \n",
      "7799  https://responsive.fxempire.com/width/600/webp...   \n",
      "7800  https://responsive.fxempire.com/width/600/webp...   \n",
      "7801  https://i-invdn-com.investing.com/redesign/ima...   \n",
      "\n",
      "                                                   body  \\\n",
      "0     [The euro has posted small gains in the Wednes...   \n",
      "1     [The New Zealand dollar continues to head high...   \n",
      "2     [By Gertrude Chavez-Dreyfuss, 3 MIN READ, NEW ...   \n",
      "3     [The February US PCE release – The Federal Res...   \n",
      "4     [By Gertrude Chavez-Dreyfuss, 3 MIN READ, NEW ...   \n",
      "...                                                 ...   \n",
      "7797  [Well, this probably isn’t going how the BoJ p...   \n",
      "7798  [European stocks followed the Asian market low...   \n",
      "7799  [TOKYO (Reuters) – A senior Japanese finance m...   \n",
      "7800  [(Reuters) – Western nations will try to find ...   \n",
      "7801  [The British pound is up for a third straight ...   \n",
      "\n",
      "                    publishedAt                    classes_target  \\\n",
      "0     2021-02-10T09:51:33-04:00                        [USD, EUR]   \n",
      "1     2021-01-21T12:22:58-04:00                        [NZD, USD]   \n",
      "2          2019-10-02T00:51:45Z         [GBP, JPY, EUR, AUD, USD]   \n",
      "3              2022-03-31T11:00                 [INDEX, USD, EUR]   \n",
      "4          2020-05-08T01:35:49Z              [CHF, JPY, USD, EUR]   \n",
      "...                         ...                               ...   \n",
      "7797           2022-04-19T14:30  [GBP, JPY, EUR, AUD, USD, INDEX]   \n",
      "7798           2022-01-18T11:37                      [INDEX, EUR]   \n",
      "7799        2022-04-23T15:08:32                        [JPY, USD]   \n",
      "7800        2022-03-31T14:20:56                             [TRY]   \n",
      "7801    Feb 01, 2022 03:21PM ET                 [GBP, INDEX, USD]   \n",
      "\n",
      "                          classes_body    classes_summary  \\\n",
      "0                           [USD, EUR]         [USD, EUR]   \n",
      "1                           [NZD, USD]         [NZD, USD]   \n",
      "2            [GBP, JPY, EUR, AUD, USD]    [JPY, USD, EUR]   \n",
      "3                    [INDEX, USD, EUR]         [USD, EUR]   \n",
      "4                 [CHF, JPY, USD, EUR]              [USD]   \n",
      "...                                ...                ...   \n",
      "7797  [GBP, JPY, EUR, AUD, USD, INDEX]              [JPY]   \n",
      "7798                      [INDEX, EUR]              [EUR]   \n",
      "7799                        [JPY, USD]              [JPY]   \n",
      "7800                             [TRY]              [TRY]   \n",
      "7801                 [GBP, INDEX, USD]  [GBP, INDEX, USD]   \n",
      "\n",
      "                  classes_title  \\\n",
      "0                         [EUR]   \n",
      "1                            []   \n",
      "2               [JPY, USD, EUR]   \n",
      "3                    [USD, EUR]   \n",
      "4                    [JPY, USD]   \n",
      "...                         ...   \n",
      "7797  [GBP, JPY, EUR, AUD, USD]   \n",
      "7798               [INDEX, EUR]   \n",
      "7799                         []   \n",
      "7800                         []   \n",
      "7801                      [GBP]   \n",
      "\n",
      "                                       summary_textrank  \\\n",
      "0     Euro gaining ground EUR/USD lost close to 1% l...   \n",
      "1     The New Zealand dollar continues to head highe...   \n",
      "2     By Gertrude Chavez-Dreyfuss  NEW YORK (Reuters...   \n",
      "3     The February US PCE release – The Federal Rese...   \n",
      "4     By Gertrude Chavez-Dreyfuss  NEW YORK (Reuters...   \n",
      "...                                                 ...   \n",
      "7797  Well, this probably isn’t going how the BoJ pl...   \n",
      "7798  European stocks followed the Asian market lowe...   \n",
      "7799  TOKYO (Reuters) – A senior Japanese finance mi...   \n",
      "7800                                                      \n",
      "7801  BoE expected to raise rates Financial markets ...   \n",
      "\n",
      "                                           summary_bert undefined  \n",
      "0     The euro has posted small gains in the Wednesd...       NaN  \n",
      "1     The New Zealand dollar continues to head highe...       NaN  \n",
      "2     The dollar dropped to a one-week low against t...       NaN  \n",
      "3     The February US PCE release – The Federal Rese...       NaN  \n",
      "4     U.S. job losses in April hit 20.5 million, com...       NaN  \n",
      "...                                                 ...       ...  \n",
      "7797   USD/JPY is now working on its 13th consecutiv...       NaN  \n",
      "7798  European stocks follow Asian market lower in e...       NaN  \n",
      "7799  A senior Japanese finance ministry official de...       NaN  \n",
      "7800  Vladimir Putin says Western nations will try t...       NaN  \n",
      "7801  The British pound is up for a third straight d...       NaN  \n",
      "\n",
      "[7802 rows x 15 columns]\n",
      "                                                   text  \\\n",
      "0     The euro has posted small gains in the Wednesd...   \n",
      "1     The New Zealand dollar continues to head highe...   \n",
      "2     The dollar on Wednesday dropped to a one-week ...   \n",
      "3     The US dollar is reclaiming some of its recent...   \n",
      "4     The U.S. dollar rose against some currencies o...   \n",
      "...                                                 ...   \n",
      "7797  The Yen meltdown has continued and this mornin...   \n",
      "7798  European stocks started to sell off amid highe...   \n",
      "7799  TOKYO (Reuters) – A senior Japanese finance mi...   \n",
      "7800  (Reuters) – Western nations will try to find n...   \n",
      "7801  The British pound is up for a third straight d...   \n",
      "\n",
      "                       classes  \n",
      "0     [0, 0, 0, 1, 0, 0, 0, 1]  \n",
      "1     [0, 0, 0, 0, 0, 0, 1, 1]  \n",
      "2     [0, 0, 0, 1, 0, 1, 0, 1]  \n",
      "3     [0, 0, 0, 1, 0, 0, 0, 1]  \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 1]  \n",
      "...                        ...  \n",
      "7797  [0, 0, 0, 0, 0, 1, 0, 0]  \n",
      "7798  [0, 0, 0, 1, 0, 0, 0, 0]  \n",
      "7799  [0, 0, 0, 0, 0, 1, 0, 0]  \n",
      "7800  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "7801  [0, 0, 0, 0, 1, 0, 0, 1]  \n",
      "\n",
      "[7802 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "ohe = pd.concat([data,data['classes_summary'].fillna(\"\").map(lambda x: \",\".join(x)).str.get_dummies(sep=\",\")],axis=1)\n",
    "to_remove = ['ARS','MXN','OIL','XAU','TRY','INDEX','BTC','TWD','RUB','CHF']\n",
    "ohe = ohe.drop(labels=to_remove,axis=1)\n",
    "classes = ohe.columns.difference(data.columns).tolist()\n",
    "data = ohe\n",
    "\n",
    "# data['sentiment'] = encoder.fit_transform((data['Sentiment']))\n",
    "# data['emotion'] = encoder.fit_transform((data['Emotion']))\n",
    "\n",
    "\n",
    "data['classes'] = list(data[classes].values)\n",
    "# classes = data['classes']\n",
    "labels = list(data.classes.values)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "# data = data.sample(n=20000)\n",
    "\n",
    "data['text'] = data['summary']\n",
    "\n",
    "data = data[['text', 'classes']]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e599b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset:\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts,\n",
    "                 labels):\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = config.MAX_LENGTH\n",
    "\n",
    "        self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\n",
    "            config.MODEL\n",
    "        )\n",
    "\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        texts = str(self.texts[item])\n",
    "        texts = \" \".join(texts.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            texts,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        true_seq_length = len(inputs['input_ids'][0])\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        ids = torch.cat((inputs['input_ids'][0], pad_ids))\n",
    "\n",
    "\n",
    "        output_dict = {\n",
    "            \"ids\": ids.flatten(),\n",
    "            'attention_mask': inputs[\"attention_mask\"][0].flatten(),\n",
    "            'target' : torch.tensor(self.labels[item], dtype=torch.long)\n",
    "\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ae8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "n_classes = len(classes)\n",
    "\n",
    "train, val = train_test_split(\n",
    "    data, test_size=0.30, random_state=SEED)\n",
    "\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3368033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.MODEL)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(config.HIDDEN, n_classes)\n",
    "\n",
    "            \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        ).pooler_output\n",
    "        output = self.drop(pooled_output)\n",
    "        # output = self.softmax(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83873fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 878k/878k [00:00<00:00, 1.26MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 446k/446k [00:00<00:00, 749kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 120kB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RobertaTokenizer' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 11\u001b[0m traindata \u001b[38;5;241m=\u001b[39m \u001b[43mTextClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m trainLoader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     17\u001b[0m     traindata, \n\u001b[0;32m     18\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     19\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTRAIN_BATCH_SIZE)\n\u001b[0;32m     22\u001b[0m valdata \u001b[38;5;241m=\u001b[39m TextClassificationDataset(\n\u001b[0;32m     23\u001b[0m     val\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mto_numpy(), \n\u001b[0;32m     24\u001b[0m     train\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     25\u001b[0m     )\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mTextClassificationDataset.__init__\u001b[1;34m(self, texts, labels)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mMAX_LENGTH\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mRobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     12\u001b[0m     config\u001b[38;5;241m.\u001b[39mMODEL\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep_vid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_vid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_vid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RobertaTokenizer' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "traindata = TextClassificationDataset(\n",
    "    train.text.to_numpy(), \n",
    "    train.classes.to_numpy()\n",
    "    )\n",
    "\n",
    "trainLoader = DataLoader(\n",
    "    traindata, \n",
    "    shuffle=True, \n",
    "    batch_size=config.TRAIN_BATCH_SIZE)\n",
    "\n",
    "\n",
    "valdata = TextClassificationDataset(\n",
    "    val.text.to_numpy(), \n",
    "    train.classes.to_numpy()\n",
    "    )\n",
    "\n",
    "valLoader = DataLoader(\n",
    "    valdata, \n",
    "    shuffle=True, \n",
    "    batch_size=config.TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aef461",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"Cuda available\")\n",
    "\n",
    "else:\n",
    "    print(\"No GPU's available\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_STEPS = len(trainLoader)*config.EPOCHS\n",
    "\n",
    "model = Model(n_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = config.LR[-1],\n",
    "    eps=config.EPS\n",
    "    )\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=TOTAL_STEPS\n",
    ")\n",
    "\n",
    "def accuracy_check(y_pred, y_true):\n",
    "    y_pred = [pl>0.5 for pl in y_pred]\n",
    "    \n",
    "    metrics = { \n",
    "                'exact': sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None),\n",
    "                'hamming':sklearn.metrics.hamming_loss(y_true, y_pred),\n",
    "                'recall': sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='samples'),\n",
    "                'precision': sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, average='samples'),\n",
    "                'f1': sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        \n",
    "                                       }\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0], metrics\n",
    "\n",
    "def format_time(elasped):\n",
    "    elasped_rounded = int(round(elasped))\n",
    "\n",
    "    return str(datetime.timedelta(seconds=elasped_rounded))\n",
    "\n",
    "random.seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "torch.manual_seed(config.SEED)\n",
    "torch.cuda.manual_seed(config.SEED)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets.type(torch.FloatTensor).to(device))\n",
    "\n",
    "for epoch in range(0, config.EPOCHS):\n",
    "    print(\"\")\n",
    "    print(\"============EPOCHS {:}/{:}=============\".format(epoch + 1, config.EPOCHS))\n",
    "    print(\"Training\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(trainLoader):\n",
    "       \n",
    "\n",
    "        if step % 40 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "\n",
    "            print(\" Batch {:>5,} of {:>5,}. Elasped: {:}\".format(step, len(trainLoader), elapsed))\n",
    "\n",
    "        b_input_ids = batch['ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['target'].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(\n",
    "            b_input_ids,\n",
    "            attention_mask = b_input_mask,\n",
    "            )\n",
    "        # print(output)\n",
    "        # output = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # print(output)\n",
    "\n",
    "        # lossfn = torch.nn.CrossEntropyLoss().to(device)\n",
    "        loss = loss_fn(output, b_labels).to(device)\n",
    "        \n",
    "        total_train_loss+=loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        avg_train_loss = total_train_loss/len(trainLoader)\n",
    "\n",
    "\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    #=========================================\n",
    "    #           Validation\n",
    "    #=========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    metrics = {'exact':0,'f1':0,'recall':0, 'hamming':0, 'precision':0}\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_step = 0\n",
    "\n",
    "    for batch in valLoader:\n",
    "\n",
    "        b_input_ids = batch['ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['target'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                attention_mask = b_input_mask,\n",
    "                )\n",
    "\n",
    "        # output = torch.nn.functional.softmax(output)\n",
    "\n",
    "        # lossfn = torch.nn.CrossEntropyLoss().to(device)\n",
    "        loss = loss_fn(output, b_labels).to(device)\n",
    "        \n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        output = output.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        acc, met = accuracy_check(output, label_ids)\n",
    "        \n",
    "        total_eval_accuracy += acc\n",
    "        metrics['exact'] += met['exact']\n",
    "        metrics['f1'] += met['f1']\n",
    "        metrics['precision'] += met['precision']\n",
    "        metrics['recall'] += met['recall']\n",
    "        metrics['hamming'] += met['hamming']\n",
    "\n",
    "        \n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy/len(valLoader)\n",
    "\n",
    "    print(\"   Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    print(\"   exact: {0:.2f}\".format(metrics['exact']/len(valLoader)))\n",
    "    print(\"   f1: {0:.2f}\".format(metrics['f1']/len(valLoader)))\n",
    "    print(\"   precision: {0:.2f}\".format(metrics['precision']/len(valLoader)))\n",
    "    print(\"   recall: {0:.2f}\".format(metrics['recall']/len(valLoader)))\n",
    "    print(\"   hamming: {0:.2f}\".format(metrics['hamming']/len(valLoader)))\n",
    "\n",
    "    avg_val_loss = total_eval_loss/len(valLoader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \"epochs\":epoch + 1,\n",
    "            \"Trainning Loss\":avg_train_loss,\n",
    "            \"Valid Loss\": avg_val_loss,\n",
    "            \"Valid Acc\": avg_val_accuracy,\n",
    "            \"Training Time\": validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training Complete\")\n",
    "print(\"Total Time took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7140fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
